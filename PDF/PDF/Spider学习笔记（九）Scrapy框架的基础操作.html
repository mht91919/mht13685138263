
<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
</head>
<body>
<div id="content_views" class="htmledit_views clearfix">
                        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mobile/css/edit_views_html-b7fdf59250.min.css">
                        <h1>Scrapy 框架</h1> 
<ul><li> <p>Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。</p> </li><li> <p>框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。</p> </li><li> <p>Scrapy 使用了 Twisted(其主要对手是Tornado)多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。</p> </li></ul>
<h1>Scrapy架构图(绿线是数据流向)</h1> 
<p> </p> 
<p> </p> 
<ul><li> <p>Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</p> </li><li> <p><code>Scheduler(调度器)</code>: 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</p> </li><li> <p><code>Downloader（下载器）</code>：负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理，</p> </li><li> <p><code>Spider（爬虫）</code>：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>，</p> </li><li> <p><code>Item Pipeline(管道)</code>：它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</p> </li><li> <p><code>Downloader Middlewares（下载中间件）</code>：你可以当作是一个可以自定义扩展下载功能的组件。</p> </li><li> <p><code>Spider Middlewares（Spider中间件）</code>：你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses;和从<code>Spider</code>出去的Requests）</p> </li></ul>
<h1>Scrapy的运作流程</h1> 
<p>代码写好，程序开始运行...</p> 
<ol><li> <p><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</p> </li><li> <p><code>Spider</code>：老大要我处理xxxx.com。</p> </li><li> <p><code>引擎</code>：你把第一个需要处理的URL给我吧。</p> </li><li> <p><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</p> </li><li> <p><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</p> </li><li> <p><code>调度器</code>：好的，正在处理你等一下。</p> </li><li> <p><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</p> </li><li> <p><code>调度器</code>：给你，这是我处理好的request</p> </li><li> <p><code>引擎</code>：Hi！下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</p> </li><li> <p><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</p> </li><li> <p><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</p> </li><li> <p><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</p> </li><li> <p><code>引擎</code>：Hi ！<code>管道</code>我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</p> </li><li> <p><code>管道``调度器</code>：好的，现在就做！</p> </li></ol>
<p><strong>注意！只有当</strong><code>调度器</code><strong>中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</strong></p> 
<h1>安装</h1> 
<pre class="has"><code class="language-html">    1、安装wheel
        pip install wheel
    2、安装lxml
​
    3、安装pyopenssl
​
    4、安装Twisted
​
    5、安装pywin32
​
    6、安装scrapy
        pip install scrapy</code></pre> 
<h1>Scrapy的安装介绍</h1> 
<p>Scrapy框架官方网址：<a href="https://legacy.gitbook.com/book/fategithub/pythonspider/edit#">http://doc.scrapy.org/en/latest</a></p> 
<p>Scrapy中文维护站点：<a href="https://legacy.gitbook.com/book/fategithub/pythonspider/edit#">http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html</a></p> 
<h3>Windows 安装方式</h3> 
<ul><li> <p>Python 2 / 3</p> </li><li> <p>升级pip版本：</p> <p><code>pip install --upgrade pip</code></p> </li><li> <p>通过pip 安装 Scrapy 框架</p> <p><code>pip install Scrapy</code></p> </li></ul>
<h3>Ubuntu 需要9.10或以上版本安装方式</h3> 
<ul><li> <p>Python 2 / 3</p> </li><li> <p>安装非Python的依赖</p> <p><code>sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev</code></p> </li><li> <p>通过pip 安装 Scrapy 框架</p> <p><code>sudo pip install scrapy</code></p> </li></ul>
<p>安装后，只要在命令终端输入 scrapy，提示类似以下结果，代表已经安装成功</p> 
<p>具体Scrapy安装流程参考：<a href="http://doc.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes">http://doc.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes</a>里面有各个平台的安装方法</p> 
<h1>制作 Scrapy 爬虫 一共需要4步：</h1> 
<ul><li> <p>新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目</p> </li><li> <p>明确目标 （编写items.py）：明确你想要抓取的目标</p> </li><li> <p>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</p> </li><li> <p>存储内容 （pipelines.py）：设计管道存储爬取内容</p> </li></ul>
<h1>入门案例</h1> 
<h2>学习目标</h2> 
<ul><li> <p>创建一个Scrapy项目</p> </li><li> <p>定义提取的结构化数据(Item)</p> </li><li> <p>编写爬取网站的 Spider 并提取出结构化数据(Item)</p> </li><li> <p>编写 Item Pipelines 来存储提取到的Item(即结构化数据)</p> </li></ul>
<h2>一. 新建项目(scrapy startproject)</h2> 
<ul><li> <p>在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，运行下列命令：</p> </li></ul>
<pre class="has"><code class="language-html">scrapy startproject mySpider</code></pre> 
<ul><li> <p>其中， mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下：</p> </li></ul>
<p> </p> 
<p>下面来简单介绍一下各个主要文件的作用：</p> 
<blockquote> 
 <p>scrapy.cfg ：项目的配置文件</p> 
 <p>mySpider/ ：项目的Python模块，将会从这里引用代码</p> 
 <p>mySpider/items.py ：项目的目标文件</p> 
 <p>mySpider/pipelines.py ：项目的管道文件</p> 
 <p>mySpider/settings.py ：项目的设置文件</p> 
 <p>mySpider/spiders/ ：存储爬虫代码目录</p> 
</blockquote> 
<h2>二、明确目标(mySpider/items.py)</h2> 
<p>我们打算抓取：<a href="http://bbs.tianya.cn/post-140-393968-1.shtml">http://bbs.tianya.cn/post-140-393968-1.shtml</a> 网站里的邮箱。</p> 
<ol><li> <p>打开mySpider目录下的items.py</p> </li><li> <p>Item 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护减少错误。</p> </li><li> <p>可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field的类属性来定义一个Item（可以理解成类似于ORM的映射关系）。</p> </li><li> <p>接下来，创建一个TianyaItem类，和构建item模型（model）。</p> </li></ol>
<pre class="has"><code class="language-html">import scrapy
​
class TianyaItem(scrapy.Item):
    email = scrapy.Field()</code></pre> 
<h2>三、制作爬虫 （spiders/itcastSpider.py）</h2> 
<p><strong>爬虫功能要分两步：</strong></p> 
<h3>1. 爬数据</h3> 
<ul><li> <p>在当前目录下输入命令</p> </li></ul>
<pre class="has"><code class="language-html">scrapy genspider mytianya "bbs.tianya.cn"</code></pre> 
<ul><li> <p>打开 mySpider/spider目录里的 mytianya .py，默认增加了下列代码:</p> </li></ul>
<pre class="has"><code class="language-html">import scrapy
import re
from tianya import items
​
​
class MytianyaSpider(scrapy.Spider):
    name = 'mytianya'
    allowed_domains = ['bbs.tianya.cn']
    start_urls = ['http://bbs.tianya.cn/post-140-393977-1.shtml']
​
​
    def parse(self, response):
        pass</code></pre> 
<p>其实也可以由我们自行创建itcast.py并编写上面的代码，只不过使用命令可以免去编写固定代码的麻烦</p> 
<p>要建立一个Spider， 你必须用scrapy.Spider类创建一个子类，并确定了三个强制的属性 和 一个方法。</p> 
<ul><li> <p><code>name = ""</code>：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</p> </li><li> <p><code>allow_domains = []</code>是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</p> </li><li> <p><code>start_urls = ()</code>：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</p> </li><li> <p><code>parse(self, response)</code>：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：</p> 
  <ol><li> <p>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</p> </li><li> <p>生成需要下一页的URL请求。</p> </li></ol></li><li> <p>将start_urls的值修改为需要爬取的第一个url</p> </li></ul>
<p>修改parse()方法</p> 
<pre class="has"><code class="language-html">    def parse(self, response):
        html = response.body.decode()
        # ftsd@21cn.com
        email = re.compile(r"([A-Z0-9_]+@[A-Z0-9]+\.[A-Z]{2,4})", re.I)
        emailList = email.findall(html)
        mydict = []
        for e in emailList:
            item = items.TianyaItem()
            item["email"] = e
            # mydict[e] = "http://bbs.tianya.cn/post-140-393977-1.shtml"
            mydict.append(item)
        return mydict</code></pre> 
<p>然后运行一下看看，在mySpider目录下执行：</p> 
<pre class="has"><code class="language-html">scrapy crawl mytianya</code></pre> 
<h2>2. 取数据</h2> 
<ul><li> <p>我们暂时先不处理管道，后面会详细介绍。</p> </li></ul>
<h2>3.保存数据</h2> 
<p>scrapy保存信息的最简单的方法主要有四种，-o 输出指定格式的文件，，命令如下：</p> 
<pre class="has"><code class="language-html">scrapy crawl mytianya -o mytianya.json
​
scrapy crawl mytianya -o mytianya.csv
​
scrapy crawl mytianya -o mytianya.xml</code></pre> 
<h2>思考</h2> 
<p>如果将代码改成下面形式，结果完全一样。</p> 
<p>请思考 yield 在这里的作用：</p> 
<p> </p> 
<pre class="has"><code class="language-html">    def parse(self, response):
        html = response.body.decode()
        # ftsd@21cn.com
        email = re.compile(r"([A-Z0-9_]+@[A-Z0-9]+.[A-Z]{2,4})", re.I)
        emailList = email.findall(html)
        mydict = []
        for e in emailList:
            item = items.TianyaItem()
            item["email"] = e
            # mydict[e] = "http://bbs.tianya.cn/post-140-393977-1.shtml"
​
​
            # mydict.append(item)
​
​
            #将获取的数据交给pipelines
            yield mydict
​
​
        # 返回数据，不经过pipeline
        return mydict</code></pre> 
<h1>Scrapy 去重</h1> 
<p> </p> 
<p> </p> 
<p>RFPDupeFilter这个类 set()集合</p> 
<p>那么在 scrapy 中是如何来使用这个类的方法的呢？什么时候使用，这个流程是怎样的呢？</p> 
<p>这个可以追溯到 scrapy.core.scheduler 中定义的 Scheduler 类来决定。</p> 
<p>现在就来看看 Scheduler 类中和过滤重复 url 有关的内容。</p> 
<p>在 Scheduler 类中，在调度时，采用了 memory queue 和 disk queue 的存储方法，所以，有一个入队的方法，在入队前，就要对 <code>request</code> 进行检查，检查是否是重复，如果已经重复了，就不入队了。</p> 
<p> </p> 
<p>这里两个条件控制，首先是配置中 dont_filter，如果它是 True，就说明是不筛选的，如果是 False，才是要筛选的。后面的 request_seen() 在默认内置的筛选方法中，就是 RFPDupeFilter() 中的方法，检查 request 是否已经存在。</p> 
<p>只有要筛选且没有见过这个 request，才会去筛选 url。</p> 
<p>所以这里已经很清晰了，调度器收到了 <code>enqueue_request()</code> 调用时，会检查这个 url 重复的判断开关，如果要筛选，就要检查这个 request 是否已经存在了；这里的检查 if 如果成立，就直接返回了，只有不成立时，才会有后续的存储操作，也就是入队。</p> 
<p> </p> 
<h3>下面来看看 scrapy 中是如何判断两个 url 重复的。</h3> 
<p>关键的函数是 <code>request_fingerprint</code>，这个是判断是否重复的关键实现方(<code>scrapy.utils.request.request_fingerprint()</code>)。</p> 
<p>默认的调用情况下，计算的内容包括 method、格式化后的 url、请求正文，还有就是 http headers 是可选的。</p> 
<p>和通常情况下不一样的是，这里的计算指纹，不是单纯的比较了 url 是否一致。计算的结果是一串 hash 16 进制数字</p> 
<h1>Scrapy Shell</h1> 
<p>Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。</p> 
<p>如果安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。（推荐安装IPython）</p> 
<h2>启动Scrapy Shell</h2> 
<p>进入项目的根目录，执行下列命令来启动shell:</p> 
<pre class="has"><code class="language-html">scrapy shell "https://hr.tencent.com/position.php?&amp;start=0#a"</code></pre> 
<p>Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及<code>Selector 对象 (对HTML及XML内容)</code>。</p> 
<ul><li> <p>当shell载入后，将得到一个包含response数据的本地 response 变量，输入<code>response.body</code>将输出response的包体，输出<code>response.headers</code>可以看到response的包头。</p> </li><li> <p>输入<code>response.selector</code>时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用<code>response.selector.xpath()</code>或<code>response.selector.css()</code>来对 response 进行查询。</p> </li><li> <p>Scrapy也提供了一些快捷方式, 例如<code>response.xpath()</code>或<code>response.css()</code>同样可以生效（如之前的案例）。</p> </li></ul>
<h2>Selectors选择器</h2> 
<p>Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制</p> 
<p>Selector有四个基本的方法，最常用的还是xpath:</p> 
<ul><li> <p>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</p> </li><li> <p>extract(): 序列化该节点为Unicode字符串并返回list</p> </li><li> <p>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4</p> </li><li> <p>re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</p> </li></ul>
<pre class="has"><code class="language-html">response.xpath('//title')</code></pre> 
<h1>Item Pipeline</h1> 
<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。</p> 
<p>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：</p> 
<ul><li> <p>验证爬取的数据(检查item包含某些字段，比如说name字段)</p> </li><li> <p>查重(并丢弃)</p> </li><li> <p>将爬取结果保存到文件或者数据库中</p> </li></ul>
<h2>编写item pipeline</h2> 
<p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:</p> 
<pre class="has"><code class="language-html">import something
​
​
class SomethingPipeline(object):
    def __init__(self):    
        # 可选实现，做参数初始化等
        # doing something
​
​
    def process_item(self, item, spider):
        # item (Item 对象) – 被爬取的item
        # spider (Spider 对象) – 爬取该item的spider
        # 这个方法必须实现，每个item pipeline组件都需要调用该方法，
        # 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。
        return item
​
​
    def open_spider(self, spider):
        # spider (Spider 对象) – 被开启的spider
        # 可选实现，当spider被开启时，这个方法被调用。
​
​
    def close_spider(self, spider):
        # spider (Spider 对象) – 被关闭的spider
        # 可选实现，当spider被关闭时，这个方法被调用</code></pre> 
<h2>完善之前的案例：</h2> 
<p>item写入txt文件</p> 
<p>以下pipeline将所有(从所有'spider'中)爬取到的item，存储到一个独立地txt文件</p> 
<pre class="has"><code class="language-html">class TianyaPipeline(object):
    def __init__(self):
        self.f = open("tianya.txt", "w", encoding="utf-8")
    def process_item(self, item, spider):
        self.f.write(str(item))
        # return item
    def __del__(self):
        self.f.close()</code></pre> 
<p>启用一个Item Pipeline组件</p> 
<p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置，就像下面这个例子:</p> 
<pre class="has"><code class="language-html">ITEM_PIPELINES = {
   'tianya.pipelines.TianyaPipeline': 300,
}</code></pre> 
<p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）</p> 
<p>重新启动爬虫</p> 
<p>将parse()方法改为4.2中最后思考中的代码，然后执行下面的命令：</p> 
<pre class="has"><code class="language-html">scrapy crawl itcast</code></pre> 
<h1>Spider</h1> 
<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p> 
<p><code>class scrapy.Spider</code>是最基本的类，所有编写的爬虫必须继承这个类。</p> 
<p>主要用到的函数及调用顺序为：</p> 
<p><code>__init__()</code>: 初始化爬虫名字和start_urls列表</p> 
<p><code>start_requests() 调用make_requests_from url()</code>:生成Requests对象交给Scrapy下载并返回response</p> 
<p><code>parse()</code>: 解析response，并返回Item或Requests（需指定回调函数）。Item传给Item pipline持久化 ， 而Requests交由Scrapy下载，并由指定的回调函数处理（默认parse())，一直进行循环，直到处理完所有的数据为止。</p> 
<p>源码参考</p> 
<pre class="has"><code class="language-html">#所有爬虫的基类，用户定义的爬虫必须从这个类继承
class Spider(object_ref):
​
​
    #定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。
    #name是spider最重要的属性，而且是必须的。
    #一般做法是以该网站(domain)(加或不加 后缀 )来命名spider。 例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite
    name = None
​
​
    #初始化，提取爬虫名字，start_ruls
    def __init__(self, name=None, **kwargs):
        if name is not None:
            self.name = name
        # 如果爬虫没有名字，中断后续操作则报错
        elif not getattr(self, 'name', None):
            raise ValueError("%s must have a name" % type(self).__name__)
​
​
        # python 对象或类型通过内置成员__dict__来存储成员信息
        self.__dict__.update(kwargs)
​
​
        #URL列表。当没有指定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。
        if not hasattr(self, 'start_urls'):
            self.start_urls = []
​
​
    # 打印Scrapy执行后的log信息
    def log(self, message, level=log.DEBUG, **kw):
        log.msg(message, spider=self, level=level, **kw)
​
​
    # 判断对象object的属性是否存在，不存在做断言处理
    def set_crawler(self, crawler):
        assert not hasattr(self, '_crawler'), "Spider already bounded to %s" % crawler
        self._crawler = crawler
​
​
    @property
    def crawler(self):
        assert hasattr(self, '_crawler'), "Spider not bounded to any crawler"
        return self._crawler
​
​
    @property
    def settings(self):
        return self.crawler.settings
​
​
    #该方法将读取start_urls内的地址，并为每一个地址生成一个Request对象，交给Scrapy下载并返回Response
    #该方法仅调用一次
    def start_requests(self):
        for url in self.start_urls:
            yield self.make_requests_from_url(url)
​
​
    #start_requests()中调用，实际生成Request的函数。
    #Request对象默认的回调函数为parse()，提交的方式为get
    def make_requests_from_url(self, url):
        return Request(url, dont_filter=True)
​
​
    #默认的Request对象回调函数，处理返回的response。
    #生成Item或者Request对象。用户必须实现这个类
    def parse(self, response):
        raise NotImplementedError
​
​
    @classmethod
    def handles_request(cls, request):
        return url_is_from_spider(request.url, cls)
​
​
    def __str__(self):
        return "&lt;%s %r at 0x%0x&gt;" % (type(self).__name__, self.name, id(self))
​
​
    __repr__ = __str__</code></pre> 
<p>主要属性和方法</p> 
<ul><li> <p>name</p> 
  <blockquote> 
   <p>定义spider名字的字符串。</p> 
   <p>例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite</p> 
  </blockquote> </li><li> <p>allowed_domains</p> 
  <blockquote> 
   <p>包含了spider允许爬取的域名(domain)的列表，可选。</p> 
  </blockquote> </li><li> <p>start_urls</p> 
  <blockquote> 
   <p>初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</p> 
  </blockquote> </li><li> <p>start_requests(self)</p> 
  <blockquote> 
   <p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。</p> 
   <p>当spider启动爬取并且未指定start_urls时，该方法被调用。</p> 
  </blockquote> </li><li> <p>parse(self, response)</p> 
  <blockquote> 
   <p>当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</p> 
  </blockquote> </li><li> <p>log(self, message[, level, component])</p> 
  <blockquote> 
   <p>使用 scrapy.log.msg() 方法记录(log)message。 更多数据请参见<a href="https://legacy.gitbook.com/book/fategithub/pythonspider/edit#">logging</a></p> 
  </blockquote> </li></ul>
<p>案例：腾讯招聘网自动翻页采集</p> 
<ul><li> <p>创建一个新的爬虫：</p> </li></ul>
<p><code>scrapy genspider tencent "tencent.com"</code></p> 
<ul><li> <p>编写items.py</p> </li></ul>
<p>获取职位名称、详细信息、</p> 
<pre class="has"><code class="language-html">class TencentItem(scrapy.Item):
    # define the fields for your item here like:
    jobTitle = scrapy.Field()
    jobCategories = scrapy.Field()
    number = scrapy.Field()
    location = scrapy.Field()
    releasetime = scrapy.Field()</code></pre> 
<ul><li> <p>编写tencent.py</p> </li></ul>
<pre class="has"><code class="language-html"># -*- coding: utf-8 -*-
import re
​
​
import scrapy
from Tencent import items
​
​
class MytencentSpider(scrapy.Spider):
    name = 'myTencent'
    allowed_domains = ['hr.tencent.com']
    start_urls = ['https://hr.tencent.com/position.php?lid=2218&amp;start=0#a']
​
​
    def parse(self, response):
        for data in response.xpath("//tr[@class=\"even\"] | //tr[@class=\"odd\"]"):
​
​
            item = items.TencentItem()
            item["jobTitle"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["jobLink"] = data.xpath("./td[1]/a/@href")[0].extract()
            item["jobCategories"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["number"] = data.xpath("./td[2]/text()")[0].extract()
            item["location"] = data.xpath("./td[3]/text()")[0].extract()
            item["releasetime"] = data.xpath("./td[4]/text()")[0].extract()
            yield item
​
​
            for i in range(1, 200):
                newurl = "https://hr.tencent.com/position.php?lid=2218&amp;start=%d#a" % (i*10)
                yield scrapy.Request(newurl, callback=self.parse)</code></pre> 
<ul><li> <p>编写pipeline.py文件</p> </li></ul>
<pre class="has"><code class="language-html">class TencentPipeline(object):
    def __init__(self):
        self.file = open("tencent.txt", "w", encoding="utf-8")
​
​
    def process_item(self, item, spider):
        line = str(item) + "\r\n"
        self.file.write(line)
        self.file.flush()
        return item
​
​
    def __del__(self):
        self.file.close()</code></pre> 
<ul><li> <p>在 setting.py 里设置ITEM_PIPELINES</p> </li></ul>
<pre class="has"><code class="language-html">ITEM_PIPELINES = {
"mySpider.pipelines.TencentJsonPipeline":300
}</code></pre> 
<ul><li> <p>执行爬虫：</p> <p><code>scrapy crawl tencent</code></p> </li></ul>
<h2>思考</h2> 
<p>请思考 parse()方法的工作机制：</p> 
<pre class="has"><code class="language-html">1. 因为使用的yield，而不是return。parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；
2. 如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。
3. scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；
4. 取尽第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；
5. parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)
6. Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）
7. 取尽之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；
8. 程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。
7. 这一切的一切，Scrapy引擎和调度器将负责到底。</code></pre> 
<h1>CrawlSpiders</h1> 
<blockquote> 
 <p>通过下面的命令可以快速创建 CrawlSpider模板 的代码：</p> 
 <p><code>scrapy genspider -t crawl tencent tencent.com</code></p> 
</blockquote> 
<p>上一个案例中，我们通过正则表达式，制作了新的url作为Request请求参数，现在我们可以换个花样...</p> 
<p><code>class scrapy.spiders.CrawlSpider</code></p> 
<p>它是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取的工作更适合。</p> 
<p>源码参考</p> 
<pre class="has"><code class="language-html">class CrawlSpider(Spider):
    rules = ()
    def __init__(self, *a, **kw):
        super(CrawlSpider, self).__init__(*a, **kw)
        self._compile_rules()
​
​
    #首先调用parse()来处理start_urls中返回的response对象
    #parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url()
    #设置了跟进标志位True
    #parse将返回item和跟进了的Request对象    
    def parse(self, response):
        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)
​
​
    #处理start_url中返回的response，需要重写
    def parse_start_url(self, response):
        return []
​
​
    def process_results(self, response, results):
        return results
​
​
    #从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回
    def _requests_to_follow(self, response):
        if not isinstance(response, HtmlResponse):
            return
        seen = set()
        #抽取之内的所有链接，只要通过任意一个'规则'，即表示合法
        for n, rule in enumerate(self._rules):
            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]
            #使用用户指定的process_links处理每个连接
            if links and rule.process_links:
                links = rule.process_links(links)
            #将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()
            for link in links:
                seen.add(link)
                #构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数
                r = Request(url=link.url, callback=self._response_downloaded)
                r.meta.update(rule=n, link_text=link.text)
                #对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request.
                yield rule.process_request(r)
​
​
    #处理通过rule提取出的连接，并返回item以及request
    def _response_downloaded(self, response):
        rule = self._rules[response.meta['rule']]
        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)
​
​
    #解析response对象，会用callback解析处理他，并返回request或Item对象
    def _parse_response(self, response, callback, cb_kwargs, follow=True):
        #首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数）
        #如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象，
        #然后再交给process_results处理。返回cb_res的一个列表
        if callback:
            #如果是parse调用的，则会解析成Request对象
            #如果是rule callback，则会解析成Item
            cb_res = callback(response, **cb_kwargs) or ()
            cb_res = self.process_results(response, cb_res)
            for requests_or_item in iterate_spider_output(cb_res):
                yield requests_or_item
​
​
        #如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象
        if follow and self._follow_links:
            #返回每个Request对象
            for request_or_item in self._requests_to_follow(response):
                yield request_or_item
​
​
    def _compile_rules(self):
        def get_method(method):
            if callable(method):
                return method
            elif isinstance(method, basestring):
                return getattr(self, method, None)
​
​
        self._rules = [copy.copy(r) for r in self.rules]
        for rule in self._rules:
            rule.callback = get_method(rule.callback)
            rule.process_links = get_method(rule.process_links)
            rule.process_request = get_method(rule.process_request)
​
​
    def set_crawler(self, crawler):
        super(CrawlSpider, self).set_crawler(crawler)
        self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)</code></pre> 
<p>CrawlSpider继承于Spider类，除了继承过来的属性外（name、allow_domains），还提供了新的属性和方法:</p> 
<h2>LinkExtractors</h2> 
<pre class="has"><code class="language-html">class scrapy.linkextractors.LinkExtractor</code></pre> 
<p>Link Extractors 的目的很简单: 提取链接｡</p> 
<p>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。</p> 
<p>Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡</p> 
<pre class="has"><code class="language-html">class scrapy.linkextractors.LinkExtractor(
    allow = (),
    deny = (),
    allow_domains = (),
    deny_domains = (),
    deny_extensions = None,
    restrict_xpaths = (),
    tags = ('a','area'),
    attrs = ('href'),
    canonicalize = True,
    unique = True,
    process_value = None
)</code></pre> 
<p>主要参数：</p> 
<ul><li> <p><code>allow</code>：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</p> </li><li> <p><code>deny</code>：与这个正则表达式(或正则表达式列表)匹配的URL一定不提取。</p> </li><li> <p><code>allow_domains</code>：会被提取的链接的domains。</p> </li><li> <p><code>deny_domains</code>：一定不会被提取链接的domains。</p> </li><li> <p><code>restrict_xpaths</code>：使用xpath表达式，和allow共同作用过滤链接。</p> </li></ul>
<h2>rules</h2> 
<p>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作。如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。</p> 
<pre class="has"><code class="language-html">class scrapy.spiders.Rule(
        link_extractor, 
        callback = None, 
        cb_kwargs = None, 
        follow = None, 
        process_links = None, 
        process_request = None
)</code></pre> 
<ul><li> <p><code>link_extractor</code>：是一个Link Extractor对象，用于定义需要提取的链接。</p> </li><li> <p><code>callback</code>： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。</p> 
  <blockquote> 
   <p>注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</p> 
  </blockquote> </li><li> <p><code>follow</code>：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</p> </li><li> <p><code>process_links</code>：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p> </li><li> <p><code>process_request</code>：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)</p> </li></ul>
<h2>爬取规则(Crawling rules)</h2> 
<p>继续用腾讯招聘为例，给出配合rule使用CrawlSpider的例子:</p> 
<ol><li> <p>首先运行</p> <pre class="has"><code class="language-html"> scrapy shell "http://hr.tencent.com/position.php?&amp;start=0#a"</code></pre> </li><li> <p>导入LinkExtractor，创建LinkExtractor实例对象。：</p> <pre class="has"><code class="language-html">from scrapy.linkextractors import LinkExtractor</code></pre> </li></ol>
<p>page_lx = LinkExtractor(allow=('position.php?&amp;start=\d+'))</p> 
<pre class="has"><code class="language-html">​
   &gt; allow : LinkExtractor对象最重要的参数之一，这是一个正则表达式，必须要匹配这个正则表达式(或正则表达式列表)的URL才会被提取，如果没有给出(或为空), 它会匹配所有的链接｡
   
   &gt; deny : 用法同allow，只不过与这个正则表达式匹配的URL不会被提取)｡它的优先级高于 allow 的参数，如果没有给出(或None), 将不排除任何链接｡
​
3. 调用LinkExtractor实例的extract_links()方法查询匹配结果：
​
   ```python
    page_lx.extract_links(response)</code></pre> 
<ol><li> <p>没有查到：</p> <pre class="has"><code class="language-html"> []</code></pre> </li><li> <p>注意转义字符的问题，继续重新匹配：</p> <pre class="has"><code class="language-html"> page_lx = LinkExtractor(allow=('position\.php\?&amp;start=\d+'))
 # page_lx = LinkExtractor(allow = ('start=\d+'))</code></pre> </li></ol>
<pre class="has"><code class="language-html">page_lx.extract_links(response)


</code></pre> 
<pre class="has"><code class="language-html">## CrawlSpider 版本

那么，scrapy shell测试完成之后，修改以下代码

​```python
#提取匹配 'http://hr.tencent.com/position.php?&amp;start=\d+'的链接
page_lx = LinkExtractor(allow = ('start=\d+'))


rules = [
    #提取匹配,并使用spider的parse方法进行分析;并跟进链接(没有callback意味着follow默认为True)
    Rule(page_lx, callback = 'parse', follow = True)
]


</code></pre> 
<p><strong>这么写对吗？</strong></p> 
<p><strong>不对！千万记住 callback 千万不能写 parse，再次强调：由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</strong></p> 
<pre class="has"><code class="language-html"># -*- coding: utf-8 -*-
import re
​
​
import scrapy
​
​
from  scrapy.spiders import CrawlSpider, Rule  # 提取超链接的规则
from  scrapy.linkextractors import LinkExtractor  # 提取超链接
​
​
from Tencent import items
​
​
​
​
class MytencentSpider(CrawlSpider):
    name = 'myTencent'
    allowed_domains = ['hr.tencent.com']
    start_urls = ['https://hr.tencent.com/position.php?lid=2218&amp;start=0#a']
​
​
    page_lx = LinkExtractor(allow=("start=\d+"))
​
​
    rules = [
        Rule(page_lx, callback="parseContent", follow=True)
    ]
​
​
    # parse(self, response)
    def parseContent(self, response):
        for data in response.xpath("//tr[@class=\"even\"] | //tr[@class=\"odd\"]"):
            item = items.TencentItem()
            item["jobTitle"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["jobLink"] = "https://hr.tencent.com/" + data.xpath("./td[1]/a/@href")[0].extract()
            item["jobCategories"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["number"] = data.xpath("./td[2]/text()")[0].extract()
            item["location"] = data.xpath("./td[3]/text()")[0].extract()
            item["releasetime"] = data.xpath("./td[4]/text()")[0].extract()
​
​
​
​
            yield item
​
​
            # for i in range(1, 200):
            #     newurl = "https://hr.tencent.com/position.php?lid=2218&amp;start=%d#a" % (i*10)
            #     yield scrapy.Request(newurl, callback=self.parse)</code></pre> 
<p>运行：<code>scrapy crawl tencent</code></p> 
<h2>robots协议</h2> 
<p>Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。robots.txt文件是一个文本文件。当一个搜索蜘蛛访问一个<a href="https://baike.baidu.com/item/%E7%AB%99%E7%82%B9">站点</a>时，它会首先<a href="https://baike.baidu.com/item/%E6%A3%80%E6%9F%A5">检查</a>该站点<a href="https://baike.baidu.com/item/%E6%A0%B9%E7%9B%AE%E5%BD%95">根目录</a>下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。</p> 
<pre class="has"><code class="language-html">User-agent: * 这里的*代表的所有的搜索引擎种类，*是一个通配符
Disallow: /admin/ 这里定义是禁止爬寻admin目录下面的目录
Disallow: /require/ 这里定义是禁止爬寻require目录下面的目录
Disallow: /ABC/ 这里定义是禁止爬寻ABC目录下面的目录
Disallow: /cgi-bin/*.htm 禁止访问/cgi-bin/目录下的所有以".htm"为后缀的URL(包含子目录)。
Disallow: /*?* 禁止访问网站中所有包含问号 (?) 的网址
Disallow: /.jpg$ 禁止抓取网页所有的.jpg格式的图片
Disallow:/ab/adc.html 禁止爬取ab文件夹下面的adc.html文件。
Allow: /cgi-bin/　这里定义是允许爬寻cgi-bin目录下面的目录
Allow: /tmp 这里定义是允许爬寻tmp的整个目录
Allow: .htm$ 仅允许访问以".htm"为后缀的URL。
Allow: .gif$ 允许抓取网页和gif格式图片
Sitemap: 网站地图 告诉爬虫这个页面是网站地图


</code></pre> 
<pre class="has"><code class="language-html">实例分析：淘宝网的 robots.txt文件


</code></pre> 
<p><strong>禁止robots协议将 ROBOTSTXT_OBEY = True改为False</strong></p> 
<p> </p> 
<h2>Logging</h2> 
<p>Scrapy提供了log功能，可以通过 logging 模块使用。</p> 
<blockquote> 
 <p>可以修改配置文件settings.py，任意位置添加下面两行，效果会清爽很多。</p> 
</blockquote> 
<pre class="has"><code class="language-html">LOG_ENABLED = True  # 开启
LOG_FILE = "TencentSpider.log" #日志文件名
LOG_LEVEL = "INFO" #日志级别</code></pre> 
<p>Log levels</p> 
<ul><li> <p>Scrapy提供5层logging级别:</p> </li><li> <p>CRITICAL - 严重错误(critical)</p> </li><li> <p>ERROR - 一般错误(regular errors)</p> </li><li> <p>WARNING - 警告信息(warning messages)</p> </li><li> <p>INFO - 一般信息(informational messages)</p> </li><li> <p>DEBUG - 调试信息(debugging messages)</p> </li></ul>
<p>logging设置</p> 
<p>通过在setting.py中进行以下设置可以被用来配置logging:</p> 
<ol><li> <p><code>LOG_ENABLED</code></p> <p>默认: True，启用logging</p> </li><li> <p><code>LOG_ENCODING</code></p> <p>默认: 'utf-8'，logging使用的编码</p> </li><li> <p><code>LOG_FILE</code></p> <p>默认: None，在当前目录里创建logging输出文件的文件名</p> </li><li> <p><code>LOG_LEVEL</code></p> <p>默认: 'DEBUG'，log的最低级别</p> </li><li> <p><code>LOG_STDOUT</code></p> <p>默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print "hello" ，其将会在Scrapy log中显示。</p> </li><li> <p>日志模块已经被scrapy弃用，改用python自带日志模块</p> </li></ol>
<pre class="has"><code class="language-html">import logging
​
LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"  # 设置输出格式
DATE_FORMAT = "%Y/%m/%d %H:%M:%S"  # 设置时间格式
logging.basicConfig(filename='tianya.log', filemode='a+', format=LOG_FORMAT, datefmt=DATE_FORMAT)
​
logging.warning('错误')</code></pre> 
<p> </p> 
<p>setting.py 设置抓取间隔</p>
                        </div>
</body>
</html>

<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
</head>
<body>
<div id="content_views" class="htmledit_views clearfix">
                        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mobile/css/edit_views_html-b7fdf59250.min.css">
                        <h1>Scrapy 框架</h1> 
<ul><li> <p>Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。</p> </li><li> <p>框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。</p> </li><li> <p>Scrapy 使用了 Twisted(其主要对手是Tornado)多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。</p> </li></ul>
<h1>Scrapy架构图(绿线是数据流向)</h1> 
<p> </p> 
<p> </p> 
<ul><li> <p>Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</p> </li><li> <p><code>Scheduler(调度器)</code>: 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</p> </li><li> <p><code>Downloader（下载器）</code>：负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理，</p> </li><li> <p><code>Spider（爬虫）</code>：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>，</p> </li><li> <p><code>Item Pipeline(管道)</code>：它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</p> </li><li> <p><code>Downloader Middlewares（下载中间件）</code>：你可以当作是一个可以自定义扩展下载功能的组件。</p> </li><li> <p><code>Spider Middlewares（Spider中间件）</code>：你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses;和从<code>Spider</code>出去的Requests）</p> </li></ul>
<h1>Scrapy的运作流程</h1> 
<p>代码写好，程序开始运行...</p> 
<ol><li> <p><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</p> </li><li> <p><code>Spider</code>：老大要我处理xxxx.com。</p> </li><li> <p><code>引擎</code>：你把第一个需要处理的URL给我吧。</p> </li><li> <p><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</p> </li><li> <p><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</p> </li><li> <p><code>调度器</code>：好的，正在处理你等一下。</p> </li><li> <p><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</p> </li><li> <p><code>调度器</code>：给你，这是我处理好的request</p> </li><li> <p><code>引擎</code>：Hi！下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</p> </li><li> <p><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</p> </li><li> <p><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</p> </li><li> <p><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</p> </li><li> <p><code>引擎</code>：Hi ！<code>管道</code>我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</p> </li><li> <p><code>管道``调度器</code>：好的，现在就做！</p> </li></ol>
<p><strong>注意！只有当</strong><code>调度器</code><strong>中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</strong></p> 
<h1>安装</h1> 
<pre class="has"><code class="language-html">    1、安装wheel
        pip install wheel
    2、安装lxml
​
    3、安装pyopenssl
​
    4、安装Twisted
​
    5、安装pywin32
​
    6、安装scrapy
        pip install scrapy</code></pre> 
<h1>Scrapy的安装介绍</h1> 
<p>Scrapy框架官方网址：<a href="https://legacy.gitbook.com/book/fategithub/pythonspider/edit#">http://doc.scrapy.org/en/latest</a></p> 
<p>Scrapy中文维护站点：<a href="https://legacy.gitbook.com/book/fategithub/pythonspider/edit#">http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html</a></p> 
<h3>Windows 安装方式</h3> 
<ul><li> <p>Python 2 / 3</p> </li><li> <p>升级pip版本：</p> <p><code>pip install --upgrade pip</code></p> </li><li> <p>通过pip 安装 Scrapy 框架</p> <p><code>pip install Scrapy</code></p> </li></ul>
<h3>Ubuntu 需要9.10或以上版本安装方式</h3> 
<ul><li> <p>Python 2 / 3</p> </li><li> <p>安装非Python的依赖</p> <p><code>sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev</code></p> </li><li> <p>通过pip 安装 Scrapy 框架</p> <p><code>sudo pip install scrapy</code></p> </li></ul>
<p>安装后，只要在命令终端输入 scrapy，提示类似以下结果，代表已经安装成功</p> 
<p>具体Scrapy安装流程参考：<a href="http://doc.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes">http://doc.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes</a>里面有各个平台的安装方法</p> 
<h1>制作 Scrapy 爬虫 一共需要4步：</h1> 
<ul><li> <p>新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目</p> </li><li> <p>明确目标 （编写items.py）：明确你想要抓取的目标</p> </li><li> <p>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</p> </li><li> <p>存储内容 （pipelines.py）：设计管道存储爬取内容</p> </li></ul>
<h1>入门案例</h1> 
<h2>学习目标</h2> 
<ul><li> <p>创建一个Scrapy项目</p> </li><li> <p>定义提取的结构化数据(Item)</p> </li><li> <p>编写爬取网站的 Spider 并提取出结构化数据(Item)</p> </li><li> <p>编写 Item Pipelines 来存储提取到的Item(即结构化数据)</p> </li></ul>
<h2>一. 新建项目(scrapy startproject)</h2> 
<ul><li> <p>在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，运行下列命令：</p> </li></ul>
<pre class="has"><code class="language-html">scrapy startproject mySpider</code></pre> 
<ul><li> <p>其中， mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下：</p> </li></ul>
<p> </p> 
<p>下面来简单介绍一下各个主要文件的作用：</p> 
<blockquote> 
 <p>scrapy.cfg ：项目的配置文件</p> 
 <p>mySpider/ ：项目的Python模块，将会从这里引用代码</p> 
 <p>mySpider/items.py ：项目的目标文件</p> 
 <p>mySpider/pipelines.py ：项目的管道文件</p> 
 <p>mySpider/settings.py ：项目的设置文件</p> 
 <p>mySpider/spiders/ ：存储爬虫代码目录</p> 
</blockquote> 
<h2>二、明确目标(mySpider/items.py)</h2> 
<p>我们打算抓取：<a href="http://bbs.tianya.cn/post-140-393968-1.shtml">http://bbs.tianya.cn/post-140-393968-1.shtml</a> 网站里的邮箱。</p> 
<ol><li> <p>打开mySpider目录下的items.py</p> </li><li> <p>Item 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护减少错误。</p> </li><li> <p>可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field的类属性来定义一个Item（可以理解成类似于ORM的映射关系）。</p> </li><li> <p>接下来，创建一个TianyaItem类，和构建item模型（model）。</p> </li></ol>
<pre class="has"><code class="language-html">import scrapy
​
class TianyaItem(scrapy.Item):
    email = scrapy.Field()</code></pre> 
<h2>三、制作爬虫 （spiders/itcastSpider.py）</h2> 
<p><strong>爬虫功能要分两步：</strong></p> 
<h3>1. 爬数据</h3> 
<ul><li> <p>在当前目录下输入命令</p> </li></ul>
<pre class="has"><code class="language-html">scrapy genspider mytianya "bbs.tianya.cn"</code></pre> 
<ul><li> <p>打开 mySpider/spider目录里的 mytianya .py，默认增加了下列代码:</p> </li></ul>
<pre class="has"><code class="language-html">import scrapy
import re
from tianya import items
​
​
class MytianyaSpider(scrapy.Spider):
    name = 'mytianya'
    allowed_domains = ['bbs.tianya.cn']
    start_urls = ['http://bbs.tianya.cn/post-140-393977-1.shtml']
​
​
    def parse(self, response):
        pass</code></pre> 
<p>其实也可以由我们自行创建itcast.py并编写上面的代码，只不过使用命令可以免去编写固定代码的麻烦</p> 
<p>要建立一个Spider， 你必须用scrapy.Spider类创建一个子类，并确定了三个强制的属性 和 一个方法。</p> 
<ul><li> <p><code>name = ""</code>：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</p> </li><li> <p><code>allow_domains = []</code>是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</p> </li><li> <p><code>start_urls = ()</code>：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</p> </li><li> <p><code>parse(self, response)</code>：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：</p> 
  <ol><li> <p>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</p> </li><li> <p>生成需要下一页的URL请求。</p> </li></ol></li><li> <p>将start_urls的值修改为需要爬取的第一个url</p> </li></ul>
<p>修改parse()方法</p> 
<pre class="has"><code class="language-html">    def parse(self, response):
        html = response.body.decode()
        # ftsd@21cn.com
        email = re.compile(r"([A-Z0-9_]+@[A-Z0-9]+\.[A-Z]{2,4})", re.I)
        emailList = email.findall(html)
        mydict = []
        for e in emailList:
            item = items.TianyaItem()
            item["email"] = e
            # mydict[e] = "http://bbs.tianya.cn/post-140-393977-1.shtml"
            mydict.append(item)
        return mydict</code></pre> 
<p>然后运行一下看看，在mySpider目录下执行：</p> 
<pre class="has"><code class="language-html">scrapy crawl mytianya</code></pre> 
<h2>2. 取数据</h2> 
<ul><li> <p>我们暂时先不处理管道，后面会详细介绍。</p> </li></ul>
<h2>3.保存数据</h2> 
<p>scrapy保存信息的最简单的方法主要有四种，-o 输出指定格式的文件，，命令如下：</p> 
<pre class="has"><code class="language-html">scrapy crawl mytianya -o mytianya.json
​
scrapy crawl mytianya -o mytianya.csv
​
scrapy crawl mytianya -o mytianya.xml</code></pre> 
<h2>思考</h2> 
<p>如果将代码改成下面形式，结果完全一样。</p> 
<p>请思考 yield 在这里的作用：</p> 
<p> </p> 
<pre class="has"><code class="language-html">    def parse(self, response):
        html = response.body.decode()
        # ftsd@21cn.com
        email = re.compile(r"([A-Z0-9_]+@[A-Z0-9]+.[A-Z]{2,4})", re.I)
        emailList = email.findall(html)
        mydict = []
        for e in emailList:
            item = items.TianyaItem()
            item["email"] = e
            # mydict[e] = "http://bbs.tianya.cn/post-140-393977-1.shtml"
​
​
            # mydict.append(item)
​
​
            #将获取的数据交给pipelines
            yield mydict
​
​
        # 返回数据，不经过pipeline
        return mydict</code></pre> 
<h1>Scrapy 去重</h1> 
<p> </p> 
<p> </p> 
<p>RFPDupeFilter这个类 set()集合</p> 
<p>那么在 scrapy 中是如何来使用这个类的方法的呢？什么时候使用，这个流程是怎样的呢？</p> 
<p>这个可以追溯到 scrapy.core.scheduler 中定义的 Scheduler 类来决定。</p> 
<p>现在就来看看 Scheduler 类中和过滤重复 url 有关的内容。</p> 
<p>在 Scheduler 类中，在调度时，采用了 memory queue 和 disk queue 的存储方法，所以，有一个入队的方法，在入队前，就要对 <code>request</code> 进行检查，检查是否是重复，如果已经重复了，就不入队了。</p> 
<p> </p> 
<p>这里两个条件控制，首先是配置中 dont_filter，如果它是 True，就说明是不筛选的，如果是 False，才是要筛选的。后面的 request_seen() 在默认内置的筛选方法中，就是 RFPDupeFilter() 中的方法，检查 request 是否已经存在。</p> 
<p>只有要筛选且没有见过这个 request，才会去筛选 url。</p> 
<p>所以这里已经很清晰了，调度器收到了 <code>enqueue_request()</code> 调用时，会检查这个 url 重复的判断开关，如果要筛选，就要检查这个 request 是否已经存在了；这里的检查 if 如果成立，就直接返回了，只有不成立时，才会有后续的存储操作，也就是入队。</p> 
<p> </p> 
<h3>下面来看看 scrapy 中是如何判断两个 url 重复的。</h3> 
<p>关键的函数是 <code>request_fingerprint</code>，这个是判断是否重复的关键实现方(<code>scrapy.utils.request.request_fingerprint()</code>)。</p> 
<p>默认的调用情况下，计算的内容包括 method、格式化后的 url、请求正文，还有就是 http headers 是可选的。</p> 
<p>和通常情况下不一样的是，这里的计算指纹，不是单纯的比较了 url 是否一致。计算的结果是一串 hash 16 进制数字</p> 
<h1>Scrapy Shell</h1> 
<p>Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。</p> 
<p>如果安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。（推荐安装IPython）</p> 
<h2>启动Scrapy Shell</h2> 
<p>进入项目的根目录，执行下列命令来启动shell:</p> 
<pre class="has"><code class="language-html">scrapy shell "https://hr.tencent.com/position.php?&amp;start=0#a"</code></pre> 
<p>Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及<code>Selector 对象 (对HTML及XML内容)</code>。</p> 
<ul><li> <p>当shell载入后，将得到一个包含response数据的本地 response 变量，输入<code>response.body</code>将输出response的包体，输出<code>response.headers</code>可以看到response的包头。</p> </li><li> <p>输入<code>response.selector</code>时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用<code>response.selector.xpath()</code>或<code>response.selector.css()</code>来对 response 进行查询。</p> </li><li> <p>Scrapy也提供了一些快捷方式, 例如<code>response.xpath()</code>或<code>response.css()</code>同样可以生效（如之前的案例）。</p> </li></ul>
<h2>Selectors选择器</h2> 
<p>Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制</p> 
<p>Selector有四个基本的方法，最常用的还是xpath:</p> 
<ul><li> <p>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</p> </li><li> <p>extract(): 序列化该节点为Unicode字符串并返回list</p> </li><li> <p>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4</p> </li><li> <p>re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</p> </li></ul>
<pre class="has"><code class="language-html">response.xpath('//title')</code></pre> 
<h1>Item Pipeline</h1> 
<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。</p> 
<p>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：</p> 
<ul><li> <p>验证爬取的数据(检查item包含某些字段，比如说name字段)</p> </li><li> <p>查重(并丢弃)</p> </li><li> <p>将爬取结果保存到文件或者数据库中</p> </li></ul>
<h2>编写item pipeline</h2> 
<p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:</p> 
<pre class="has"><code class="language-html">import something
​
​
class SomethingPipeline(object):
    def __init__(self):    
        # 可选实现，做参数初始化等
        # doing something
​
​
    def process_item(self, item, spider):
        # item (Item 对象) – 被爬取的item
        # spider (Spider 对象) – 爬取该item的spider
        # 这个方法必须实现，每个item pipeline组件都需要调用该方法，
        # 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。
        return item
​
​
    def open_spider(self, spider):
        # spider (Spider 对象) – 被开启的spider
        # 可选实现，当spider被开启时，这个方法被调用。
​
​
    def close_spider(self, spider):
        # spider (Spider 对象) – 被关闭的spider
        # 可选实现，当spider被关闭时，这个方法被调用</code></pre> 
<h2>完善之前的案例：</h2> 
<p>item写入txt文件</p> 
<p>以下pipeline将所有(从所有'spider'中)爬取到的item，存储到一个独立地txt文件</p> 
<pre class="has"><code class="language-html">class TianyaPipeline(object):
    def __init__(self):
        self.f = open("tianya.txt", "w", encoding="utf-8")
    def process_item(self, item, spider):
        self.f.write(str(item))
        # return item
    def __del__(self):
        self.f.close()</code></pre> 
<p>启用一个Item Pipeline组件</p> 
<p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置，就像下面这个例子:</p> 
<pre class="has"><code class="language-html">ITEM_PIPELINES = {
   'tianya.pipelines.TianyaPipeline': 300,
}</code></pre> 
<p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）</p> 
<p>重新启动爬虫</p> 
<p>将parse()方法改为4.2中最后思考中的代码，然后执行下面的命令：</p> 
<pre class="has"><code class="language-html">scrapy crawl itcast</code></pre> 
<h1>Spider</h1> 
<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p> 
<p><code>class scrapy.Spider</code>是最基本的类，所有编写的爬虫必须继承这个类。</p> 
<p>主要用到的函数及调用顺序为：</p> 
<p><code>__init__()</code>: 初始化爬虫名字和start_urls列表</p> 
<p><code>start_requests() 调用make_requests_from url()</code>:生成Requests对象交给Scrapy下载并返回response</p> 
<p><code>parse()</code>: 解析response，并返回Item或Requests（需指定回调函数）。Item传给Item pipline持久化 ， 而Requests交由Scrapy下载，并由指定的回调函数处理（默认parse())，一直进行循环，直到处理完所有的数据为止。</p> 
<p>源码参考</p> 
<pre class="has"><code class="language-html">#所有爬虫的基类，用户定义的爬虫必须从这个类继承
class Spider(object_ref):
​
​
    #定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。
    #name是spider最重要的属性，而且是必须的。
    #一般做法是以该网站(domain)(加或不加 后缀 )来命名spider。 例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite
    name = None
​
​
    #初始化，提取爬虫名字，start_ruls
    def __init__(self, name=None, **kwargs):
        if name is not None:
            self.name = name
        # 如果爬虫没有名字，中断后续操作则报错
        elif not getattr(self, 'name', None):
            raise ValueError("%s must have a name" % type(self).__name__)
​
​
        # python 对象或类型通过内置成员__dict__来存储成员信息
        self.__dict__.update(kwargs)
​
​
        #URL列表。当没有指定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。
        if not hasattr(self, 'start_urls'):
            self.start_urls = []
​
​
    # 打印Scrapy执行后的log信息
    def log(self, message, level=log.DEBUG, **kw):
        log.msg(message, spider=self, level=level, **kw)
​
​
    # 判断对象object的属性是否存在，不存在做断言处理
    def set_crawler(self, crawler):
        assert not hasattr(self, '_crawler'), "Spider already bounded to %s" % crawler
        self._crawler = crawler
​
​
    @property
    def crawler(self):
        assert hasattr(self, '_crawler'), "Spider not bounded to any crawler"
        return self._crawler
​
​
    @property
    def settings(self):
        return self.crawler.settings
​
​
    #该方法将读取start_urls内的地址，并为每一个地址生成一个Request对象，交给Scrapy下载并返回Response
    #该方法仅调用一次
    def start_requests(self):
        for url in self.start_urls:
            yield self.make_requests_from_url(url)
​
​
    #start_requests()中调用，实际生成Request的函数。
    #Request对象默认的回调函数为parse()，提交的方式为get
    def make_requests_from_url(self, url):
        return Request(url, dont_filter=True)
​
​
    #默认的Request对象回调函数，处理返回的response。
    #生成Item或者Request对象。用户必须实现这个类
    def parse(self, response):
        raise NotImplementedError
​
​
    @classmethod
    def handles_request(cls, request):
        return url_is_from_spider(request.url, cls)
​
​
    def __str__(self):
        return "&lt;%s %r at 0x%0x&gt;" % (type(self).__name__, self.name, id(self))
​
​
    __repr__ = __str__</code></pre> 
<p>主要属性和方法</p> 
<ul><li> <p>name</p> 
  <blockquote> 
   <p>定义spider名字的字符串。</p> 
   <p>例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite</p> 
  </blockquote> </li><li> <p>allowed_domains</p> 
  <blockquote> 
   <p>包含了spider允许爬取的域名(domain)的列表，可选。</p> 
  </blockquote> </li><li> <p>start_urls</p> 
  <blockquote> 
   <p>初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</p> 
  </blockquote> </li><li> <p>start_requests(self)</p> 
  <blockquote> 
   <p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。</p> 
   <p>当spider启动爬取并且未指定start_urls时，该方法被调用。</p> 
  </blockquote> </li><li> <p>parse(self, response)</p> 
  <blockquote> 
   <p>当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</p> 
  </blockquote> </li><li> <p>log(self, message[, level, component])</p> 
  <blockquote> 
   <p>使用 scrapy.log.msg() 方法记录(log)message。 更多数据请参见<a href="https://legacy.gitbook.com/book/fategithub/pythonspider/edit#">logging</a></p> 
  </blockquote> </li></ul>
<p>案例：腾讯招聘网自动翻页采集</p> 
<ul><li> <p>创建一个新的爬虫：</p> </li></ul>
<p><code>scrapy genspider tencent "tencent.com"</code></p> 
<ul><li> <p>编写items.py</p> </li></ul>
<p>获取职位名称、详细信息、</p> 
<pre class="has"><code class="language-html">class TencentItem(scrapy.Item):
    # define the fields for your item here like:
    jobTitle = scrapy.Field()
    jobCategories = scrapy.Field()
    number = scrapy.Field()
    location = scrapy.Field()
    releasetime = scrapy.Field()</code></pre> 
<ul><li> <p>编写tencent.py</p> </li></ul>
<pre class="has"><code class="language-html"># -*- coding: utf-8 -*-
import re
​
​
import scrapy
from Tencent import items
​
​
class MytencentSpider(scrapy.Spider):
    name = 'myTencent'
    allowed_domains = ['hr.tencent.com']
    start_urls = ['https://hr.tencent.com/position.php?lid=2218&amp;start=0#a']
​
​
    def parse(self, response):
        for data in response.xpath("//tr[@class=\"even\"] | //tr[@class=\"odd\"]"):
​
​
            item = items.TencentItem()
            item["jobTitle"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["jobLink"] = data.xpath("./td[1]/a/@href")[0].extract()
            item["jobCategories"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["number"] = data.xpath("./td[2]/text()")[0].extract()
            item["location"] = data.xpath("./td[3]/text()")[0].extract()
            item["releasetime"] = data.xpath("./td[4]/text()")[0].extract()
            yield item
​
​
            for i in range(1, 200):
                newurl = "https://hr.tencent.com/position.php?lid=2218&amp;start=%d#a" % (i*10)
                yield scrapy.Request(newurl, callback=self.parse)</code></pre> 
<ul><li> <p>编写pipeline.py文件</p> </li></ul>
<pre class="has"><code class="language-html">class TencentPipeline(object):
    def __init__(self):
        self.file = open("tencent.txt", "w", encoding="utf-8")
​
​
    def process_item(self, item, spider):
        line = str(item) + "\r\n"
        self.file.write(line)
        self.file.flush()
        return item
​
​
    def __del__(self):
        self.file.close()</code></pre> 
<ul><li> <p>在 setting.py 里设置ITEM_PIPELINES</p> </li></ul>
<pre class="has"><code class="language-html">ITEM_PIPELINES = {
"mySpider.pipelines.TencentJsonPipeline":300
}</code></pre> 
<ul><li> <p>执行爬虫：</p> <p><code>scrapy crawl tencent</code></p> </li></ul>
<h2>思考</h2> 
<p>请思考 parse()方法的工作机制：</p> 
<pre class="has"><code class="language-html">1. 因为使用的yield，而不是return。parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；
2. 如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。
3. scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；
4. 取尽第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；
5. parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)
6. Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）
7. 取尽之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；
8. 程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。
7. 这一切的一切，Scrapy引擎和调度器将负责到底。</code></pre> 
<h1>CrawlSpiders</h1> 
<blockquote> 
 <p>通过下面的命令可以快速创建 CrawlSpider模板 的代码：</p> 
 <p><code>scrapy genspider -t crawl tencent tencent.com</code></p> 
</blockquote> 
<p>上一个案例中，我们通过正则表达式，制作了新的url作为Request请求参数，现在我们可以换个花样...</p> 
<p><code>class scrapy.spiders.CrawlSpider</code></p> 
<p>它是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取的工作更适合。</p> 
<p>源码参考</p> 
<pre class="has"><code class="language-html">class CrawlSpider(Spider):
    rules = ()
    def __init__(self, *a, **kw):
        super(CrawlSpider, self).__init__(*a, **kw)
        self._compile_rules()
​
​
    #首先调用parse()来处理start_urls中返回的response对象
    #parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url()
    #设置了跟进标志位True
    #parse将返回item和跟进了的Request对象    
    def parse(self, response):
        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)
​
​
    #处理start_url中返回的response，需要重写
    def parse_start_url(self, response):
        return []
​
​
    def process_results(self, response, results):
        return results
​
​
    #从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回
    def _requests_to_follow(self, response):
        if not isinstance(response, HtmlResponse):
            return
        seen = set()
        #抽取之内的所有链接，只要通过任意一个'规则'，即表示合法
        for n, rule in enumerate(self._rules):
            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]
            #使用用户指定的process_links处理每个连接
            if links and rule.process_links:
                links = rule.process_links(links)
            #将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()
            for link in links:
                seen.add(link)
                #构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数
                r = Request(url=link.url, callback=self._response_downloaded)
                r.meta.update(rule=n, link_text=link.text)
                #对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request.
                yield rule.process_request(r)
​
​
    #处理通过rule提取出的连接，并返回item以及request
    def _response_downloaded(self, response):
        rule = self._rules[response.meta['rule']]
        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)
​
​
    #解析response对象，会用callback解析处理他，并返回request或Item对象
    def _parse_response(self, response, callback, cb_kwargs, follow=True):
        #首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数）
        #如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象，
        #然后再交给process_results处理。返回cb_res的一个列表
        if callback:
            #如果是parse调用的，则会解析成Request对象
            #如果是rule callback，则会解析成Item
            cb_res = callback(response, **cb_kwargs) or ()
            cb_res = self.process_results(response, cb_res)
            for requests_or_item in iterate_spider_output(cb_res):
                yield requests_or_item
​
​
        #如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象
        if follow and self._follow_links:
            #返回每个Request对象
            for request_or_item in self._requests_to_follow(response):
                yield request_or_item
​
​
    def _compile_rules(self):
        def get_method(method):
            if callable(method):
                return method
            elif isinstance(method, basestring):
                return getattr(self, method, None)
​
​
        self._rules = [copy.copy(r) for r in self.rules]
        for rule in self._rules:
            rule.callback = get_method(rule.callback)
            rule.process_links = get_method(rule.process_links)
            rule.process_request = get_method(rule.process_request)
​
​
    def set_crawler(self, crawler):
        super(CrawlSpider, self).set_crawler(crawler)
        self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)</code></pre> 
<p>CrawlSpider继承于Spider类，除了继承过来的属性外（name、allow_domains），还提供了新的属性和方法:</p> 
<h2>LinkExtractors</h2> 
<pre class="has"><code class="language-html">class scrapy.linkextractors.LinkExtractor</code></pre> 
<p>Link Extractors 的目的很简单: 提取链接｡</p> 
<p>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。</p> 
<p>Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡</p> 
<pre class="has"><code class="language-html">class scrapy.linkextractors.LinkExtractor(
    allow = (),
    deny = (),
    allow_domains = (),
    deny_domains = (),
    deny_extensions = None,
    restrict_xpaths = (),
    tags = ('a','area'),
    attrs = ('href'),
    canonicalize = True,
    unique = True,
    process_value = None
)</code></pre> 
<p>主要参数：</p> 
<ul><li> <p><code>allow</code>：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</p> </li><li> <p><code>deny</code>：与这个正则表达式(或正则表达式列表)匹配的URL一定不提取。</p> </li><li> <p><code>allow_domains</code>：会被提取的链接的domains。</p> </li><li> <p><code>deny_domains</code>：一定不会被提取链接的domains。</p> </li><li> <p><code>restrict_xpaths</code>：使用xpath表达式，和allow共同作用过滤链接。</p> </li></ul>
<h2>rules</h2> 
<p>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作。如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。</p> 
<pre class="has"><code class="language-html">class scrapy.spiders.Rule(
        link_extractor, 
        callback = None, 
        cb_kwargs = None, 
        follow = None, 
        process_links = None, 
        process_request = None
)</code></pre> 
<ul><li> <p><code>link_extractor</code>：是一个Link Extractor对象，用于定义需要提取的链接。</p> </li><li> <p><code>callback</code>： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。</p> 
  <blockquote> 
   <p>注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</p> 
  </blockquote> </li><li> <p><code>follow</code>：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</p> </li><li> <p><code>process_links</code>：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p> </li><li> <p><code>process_request</code>：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)</p> </li></ul>
<h2>爬取规则(Crawling rules)</h2> 
<p>继续用腾讯招聘为例，给出配合rule使用CrawlSpider的例子:</p> 
<ol><li> <p>首先运行</p> <pre class="has"><code class="language-html"> scrapy shell "http://hr.tencent.com/position.php?&amp;start=0#a"</code></pre> </li><li> <p>导入LinkExtractor，创建LinkExtractor实例对象。：</p> <pre class="has"><code class="language-html">from scrapy.linkextractors import LinkExtractor</code></pre> </li></ol>
<p>page_lx = LinkExtractor(allow=('position.php?&amp;start=\d+'))</p> 
<pre class="has"><code class="language-html">​
   &gt; allow : LinkExtractor对象最重要的参数之一，这是一个正则表达式，必须要匹配这个正则表达式(或正则表达式列表)的URL才会被提取，如果没有给出(或为空), 它会匹配所有的链接｡
   
   &gt; deny : 用法同allow，只不过与这个正则表达式匹配的URL不会被提取)｡它的优先级高于 allow 的参数，如果没有给出(或None), 将不排除任何链接｡
​
3. 调用LinkExtractor实例的extract_links()方法查询匹配结果：
​
   ```python
    page_lx.extract_links(response)</code></pre> 
<ol><li> <p>没有查到：</p> <pre class="has"><code class="language-html"> []</code></pre> </li><li> <p>注意转义字符的问题，继续重新匹配：</p> <pre class="has"><code class="language-html"> page_lx = LinkExtractor(allow=('position\.php\?&amp;start=\d+'))
 # page_lx = LinkExtractor(allow = ('start=\d+'))</code></pre> </li></ol>
<pre class="has"><code class="language-html">page_lx.extract_links(response)


</code></pre> 
<pre class="has"><code class="language-html">## CrawlSpider 版本

那么，scrapy shell测试完成之后，修改以下代码

​```python
#提取匹配 'http://hr.tencent.com/position.php?&amp;start=\d+'的链接
page_lx = LinkExtractor(allow = ('start=\d+'))


rules = [
    #提取匹配,并使用spider的parse方法进行分析;并跟进链接(没有callback意味着follow默认为True)
    Rule(page_lx, callback = 'parse', follow = True)
]


</code></pre> 
<p><strong>这么写对吗？</strong></p> 
<p><strong>不对！千万记住 callback 千万不能写 parse，再次强调：由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</strong></p> 
<pre class="has"><code class="language-html"># -*- coding: utf-8 -*-
import re
​
​
import scrapy
​
​
from  scrapy.spiders import CrawlSpider, Rule  # 提取超链接的规则
from  scrapy.linkextractors import LinkExtractor  # 提取超链接
​
​
from Tencent import items
​
​
​
​
class MytencentSpider(CrawlSpider):
    name = 'myTencent'
    allowed_domains = ['hr.tencent.com']
    start_urls = ['https://hr.tencent.com/position.php?lid=2218&amp;start=0#a']
​
​
    page_lx = LinkExtractor(allow=("start=\d+"))
​
​
    rules = [
        Rule(page_lx, callback="parseContent", follow=True)
    ]
​
​
    # parse(self, response)
    def parseContent(self, response):
        for data in response.xpath("//tr[@class=\"even\"] | //tr[@class=\"odd\"]"):
            item = items.TencentItem()
            item["jobTitle"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["jobLink"] = "https://hr.tencent.com/" + data.xpath("./td[1]/a/@href")[0].extract()
            item["jobCategories"] = data.xpath("./td[1]/a/text()")[0].extract()
            item["number"] = data.xpath("./td[2]/text()")[0].extract()
            item["location"] = data.xpath("./td[3]/text()")[0].extract()
            item["releasetime"] = data.xpath("./td[4]/text()")[0].extract()
​
​
​
​
            yield item
​
​
            # for i in range(1, 200):
            #     newurl = "https://hr.tencent.com/position.php?lid=2218&amp;start=%d#a" % (i*10)
            #     yield scrapy.Request(newurl, callback=self.parse)</code></pre> 
<p>运行：<code>scrapy crawl tencent</code></p> 
<h2>robots协议</h2> 
<p>Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。robots.txt文件是一个文本文件。当一个搜索蜘蛛访问一个<a href="https://baike.baidu.com/item/%E7%AB%99%E7%82%B9">站点</a>时，它会首先<a href="https://baike.baidu.com/item/%E6%A3%80%E6%9F%A5">检查</a>该站点<a href="https://baike.baidu.com/item/%E6%A0%B9%E7%9B%AE%E5%BD%95">根目录</a>下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。</p> 
<pre class="has"><code class="language-html">User-agent: * 这里的*代表的所有的搜索引擎种类，*是一个通配符
Disallow: /admin/ 这里定义是禁止爬寻admin目录下面的目录
Disallow: /require/ 这里定义是禁止爬寻require目录下面的目录
Disallow: /ABC/ 这里定义是禁止爬寻ABC目录下面的目录
Disallow: /cgi-bin/*.htm 禁止访问/cgi-bin/目录下的所有以".htm"为后缀的URL(包含子目录)。
Disallow: /*?* 禁止访问网站中所有包含问号 (?) 的网址
Disallow: /.jpg$ 禁止抓取网页所有的.jpg格式的图片
Disallow:/ab/adc.html 禁止爬取ab文件夹下面的adc.html文件。
Allow: /cgi-bin/　这里定义是允许爬寻cgi-bin目录下面的目录
Allow: /tmp 这里定义是允许爬寻tmp的整个目录
Allow: .htm$ 仅允许访问以".htm"为后缀的URL。
Allow: .gif$ 允许抓取网页和gif格式图片
Sitemap: 网站地图 告诉爬虫这个页面是网站地图


</code></pre> 
<pre class="has"><code class="language-html">实例分析：淘宝网的 robots.txt文件


</code></pre> 
<p><strong>禁止robots协议将 ROBOTSTXT_OBEY = True改为False</strong></p> 
<p> </p> 
<h2>Logging</h2> 
<p>Scrapy提供了log功能，可以通过 logging 模块使用。</p> 
<blockquote> 
 <p>可以修改配置文件settings.py，任意位置添加下面两行，效果会清爽很多。</p> 
</blockquote> 
<pre class="has"><code class="language-html">LOG_ENABLED = True  # 开启
LOG_FILE = "TencentSpider.log" #日志文件名
LOG_LEVEL = "INFO" #日志级别</code></pre> 
<p>Log levels</p> 
<ul><li> <p>Scrapy提供5层logging级别:</p> </li><li> <p>CRITICAL - 严重错误(critical)</p> </li><li> <p>ERROR - 一般错误(regular errors)</p> </li><li> <p>WARNING - 警告信息(warning messages)</p> </li><li> <p>INFO - 一般信息(informational messages)</p> </li><li> <p>DEBUG - 调试信息(debugging messages)</p> </li></ul>
<p>logging设置</p> 
<p>通过在setting.py中进行以下设置可以被用来配置logging:</p> 
<ol><li> <p><code>LOG_ENABLED</code></p> <p>默认: True，启用logging</p> </li><li> <p><code>LOG_ENCODING</code></p> <p>默认: 'utf-8'，logging使用的编码</p> </li><li> <p><code>LOG_FILE</code></p> <p>默认: None，在当前目录里创建logging输出文件的文件名</p> </li><li> <p><code>LOG_LEVEL</code></p> <p>默认: 'DEBUG'，log的最低级别</p> </li><li> <p><code>LOG_STDOUT</code></p> <p>默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print "hello" ，其将会在Scrapy log中显示。</p> </li><li> <p>日志模块已经被scrapy弃用，改用python自带日志模块</p> </li></ol>
<pre class="has"><code class="language-html">import logging
​
LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"  # 设置输出格式
DATE_FORMAT = "%Y/%m/%d %H:%M:%S"  # 设置时间格式
logging.basicConfig(filename='tianya.log', filemode='a+', format=LOG_FORMAT, datefmt=DATE_FORMAT)
​
logging.warning('错误')</code></pre> 
<p> </p> 
<p>setting.py 设置抓取间隔</p>
                        </div>
</body>
</html>
